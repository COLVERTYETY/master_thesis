
\section{Introduction}

\subsection{Background}

In 2007, the launch of the iPhone revolutionized Human-Computer Interaction (HCI) by placing a powerful, multifunctional device directly in users' pockets, transforming everyday interactions with technology.
A cornerstone of this transformation was the iPhone's virtual keyboard, which employed predictive text algorithms to enable fast and effective typing on a small screen.
This innovation not only redefined how users interacted with mobile devices but also marked an early, impactful integration of algorithmic intelligence into consumer technology, enabling seamless usability in compact form factors.

Since then, Artificial Intelligence (AI) has advanced significantly, driven by deep learning breakthroughs and more affordable computational resources.
Today, Human-Computer Interaction is no longer bound to small screens; it has extended into the physical world through augmented reality (AR).
This shift from static screens to immersive, real-world interactions redefines how we engage with technology, supporting users’ cognitive processes by making technology an extension of their thoughts.

Modern AI-driven tools like AR create seamless connections between virtual and physical spaces, challenging traditional models of interaction.
These interfaces demand a fresh understanding of how intelligent environments can support and expand users' cognitive capacities.


\subsection{Research Domain}

This thesis investigates the new affordances introduced by AI-driven AR applications within HCI, focusing on how these interfaces extend users' cognitive capacities in creative and complex tasks.
Unlike traditional screen-based interaction models, AI-enabled AR applications integrate virtual and physical spaces, creating low-signaling, high-possibility interfaces where minimal user input leads to expansive, context-rich outcomes.

Grounded in the extended mind theory\cite{andy1998extended}, which posits that tools and spaces are integral to human thought processes, this research explores how AI-driven AR can act as an extension of users’ cognition.
By embedding AI assistance directly into the physical environment, these interfaces reduce the cognitive load typically required in complex interactions, allowing users to focus more on creative and strategic aspects of their tasks.
This approach moves beyond simple usability, positioning AR as a cognitive tool that enhances users' abilities to think, create, and collaborate in ways traditional HCI methods cannot.

A key focus of this thesis is on designing AR experiences that operate independently of specialized hardware, like headsets or smart glasses.
Instead, it emphasizes the creation of applications that extend beyond the screen.
This inclusive approach allows for shared, immersive experiences that are accessible to a wider audience, including those who may not interact directly with the interface but are nonetheless part of the environment.

Through this lens, AI-driven AR transforms not only individual interactions but also the landscape of collaborative engagement.
These tools facilitate a model of interaction where technology is a flexible, intelligent extension of the user's mind rather than a separate entity.
This thesis seeks to provide a comprehensive understanding of these affordances, their practical applications, and their impact on the future of HCI, demonstrating how AI-driven AR can redefine the boundaries of digital and physical interaction.

\subsection{Problem Statement}

Current Human-Computer Interfaces lack the adaptability and cognitive support needed for seamless, collaborative engagement with AI.
Traditional AR systems often require specialized hardware and extensive user input, which can limit accessibility, increase cognitive load, and constrain creativity.
These limitations restrict AR’s potential to act as an intuitive extension of users' cognitive processes, inhibiting its effectiveness in complex, dynamic tasks.

This thesis addresses the need for AI-driven AR interfaces that function as flexible cognitive extensions, allowing users to accomplish complex tasks with minimal, intuitive input.
By focusing on low-signaling, high-possibility affordances, this research envisions AR as a tool that enhances cognitive and creative capacities.%, fostering responsive, inclusive, and contextually adaptive AI-augmented environments that expand traditional boundaries of interaction in HCI.


\subsection{Research Questions}

To deepen our understanding of AI-enabled affordances in augmented reality, this research investigates the following questions:

\begin{itemize}
    \item \textbf{1. What challenges are associated with designing for low-signaling, high-possibility affordances?}
    \item \textbf{2. How can users benefit from AR to effectively collaborate with an AI system?}
    \item \textbf{3. How do AI-driven affordances differ from traditional affordances?}
\end{itemize}

\subsection{Objectives}

In response to the above questions, this research sets forth the following objectives:

\begin{itemize}
    \item \textbf{1. To design systems that enable users to collaborate with AI to accomplish complex creative tasks.}
    \item \textbf{2. To explore how AI can enhance cognitive and creative capabilities.}
    \item \textbf{3. To develop real-time, dynamic AI-enabled AR applications.}
\end{itemize}


\subsection{Projects Overview and Contributions}

The two projects at the core of this research are the AI-Totem and the LLM Whiteboard.
Each project explores facets of AI-enabled affordances in augmented reality.
These projects illustrate how AI and AR can combine to create dynamic, collaborative experiences that embody low-signaling, high-possibility interfaces.

\subsubsection{Totem Project}

The Totem platform investigates the integration of AI with AR in digital art creation.
Using SDXL Turbo, a state-of-the-art diffusion model, the platform augments user sketches into intricate portraits, while the First Order Motion model animates these creations, allowing users to manipulate digital art with movements in real time. 
This project embodies the thesis’s objective to empower creative capabilities through intuitive and expressive HCI interactions.
The Totem enables users to explore digital art beyond the traditional screen, as their physical movements directly influence the generated images.
The result is a seamless, dynamic, embodied creative process that exemplifies how AI-driven affordances enable users to accomplish more with fewer actions.

\subsubsection{LLM White Board}

The LLM Whiteboard project facilitates user collaboration with a large language model in a live programming environment.
Designed to expand the understanding of physical computing concepts through AR, this project allows users to interact with a spatially aware LLM.
Using camera feeds to capture the physical environment, models interpret data while the LLM generates code based on high-level user commands.
The Whiteboard environment functions either as a blank canvas or as an overlay on a live camera feed, which enables users to experiment with physical computing concepts in a virtual space. 
This project aligns with the research objective of exploring collaborative AI applications, demonstrating how AI-driven affordances can enable creative coding processes with minimal signaling and high interpretive potential.

\subsection{Significance}

The Totem platform demonstrates how blending AI with AR can expand creative agency, allowing users to interact with digital art intuitively and dynamically.
This experience illustrates AI’s potential to redefine affordances within HCI, empowering users to accomplish complex creative tasks through minimal, natural gestures.
The Totem showcases how AI-driven AR can translate physical movements into digital influence, making it a compelling model for embedding physical interactions within real-time generative art.
This capability has broad implications for digital art, entertainment, and interactive media education.

Similarly, the LLM Whiteboard illustrates the power of collaborative coding and physical computing in an AR environment.
By leveraging AI’s ability to interpret spatial data and generate code based on user commands, the Whiteboard creates a setting where digital elements seamlessly extend into the user’s physical space.
This approach provides a versatile tool for multi-modal tasks that would otherwise require specialized training or complex setup, underscoring its potential applications in educational technology, rapid prototyping, and digital fabrication.

Together, the Totem and the LLM Whiteboard highlight the transformative potential of AI-driven AR to extend cognitive capabilities and facilitate productive engagement across varied applications.
They suggest a future in which AR platforms transcend traditional screen-based interaction, instead creating environments that integrate naturally with users’ physical contexts.
As models of high-possibility, low-signaling interfaces, these projects demonstrate the potential of AI-driven AR to empower users to think, create, and collaborate with AI as intuitive extensions of their own thought processes.%, setting an example for more immersive, flexible, and human-centered interaction paradigms.
